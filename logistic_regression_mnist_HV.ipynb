{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSimple logistic regression model to solve OCR task \\nwith MNIST in TensorFlow\\nMNIST dataset: yann.lecun.com/exdb/mnist/\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDemonstrate that you can run the code successfully:\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demonstrate that you can run the code successfully:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.2927976197851843\n",
      "Average loss epoch 1: 0.7335835561885701\n",
      "Average loss epoch 2: 0.6011153310189992\n",
      "Average loss epoch 3: 0.537639180680255\n",
      "Average loss epoch 4: 0.4981198023249219\n",
      "Average loss epoch 5: 0.4713153370888361\n",
      "Average loss epoch 6: 0.4514703244179279\n",
      "Average loss epoch 7: 0.43580652443401185\n",
      "Average loss epoch 8: 0.42452770530621764\n",
      "Average loss epoch 9: 0.41275305914017424\n",
      "Average loss epoch 10: 0.40479865399273957\n",
      "Average loss epoch 11: 0.39782422735835565\n",
      "Average loss epoch 12: 0.390533910556273\n",
      "Average loss epoch 13: 0.38275307562801386\n",
      "Average loss epoch 14: 0.3801540880005954\n",
      "Average loss epoch 15: 0.3755026282547237\n",
      "Average loss epoch 16: 0.3707759644671198\n",
      "Average loss epoch 17: 0.36584016969014993\n",
      "Average loss epoch 18: 0.3636669737128389\n",
      "Average loss epoch 19: 0.3585291533689677\n",
      "Average loss epoch 20: 0.3577567319144736\n",
      "Average loss epoch 21: 0.3531412124842197\n",
      "Average loss epoch 22: 0.3520789409017229\n",
      "Average loss epoch 23: 0.3482626695663501\n",
      "Average loss epoch 24: 0.34611147461515485\n",
      "Average loss epoch 25: 0.3448392283735853\n",
      "Average loss epoch 26: 0.34294661883012956\n",
      "Average loss epoch 27: 0.34076655470268985\n",
      "Average loss epoch 28: 0.3375679445919735\n",
      "Average loss epoch 29: 0.3372581414965205\n",
      "Total time: 10.555136680603027 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9113\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()\n",
    "#After running the code, I go to cygwin and run \n",
    "#Devon@RunDMC /cygdrive/c/users/Devon/Downloads\n",
    "#$ tensorboard --logdir logistic_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vary parameter batch_size through values: 8, 64, 128, 256 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 0.5171991331203417\n",
      "Average loss epoch 1: 0.3536743478445844\n",
      "Average loss epoch 2: 0.3271519891868261\n",
      "Average loss epoch 3: 0.31324189642965794\n",
      "Average loss epoch 4: 0.30439841051189737\n",
      "Average loss epoch 5: 0.2978218555323102\n",
      "Average loss epoch 6: 0.2929017872501503\n",
      "Average loss epoch 7: 0.28872523968646474\n",
      "Average loss epoch 8: 0.2853145315257663\n",
      "Average loss epoch 9: 0.2827185699361292\n",
      "Average loss epoch 10: 0.28053106770833786\n",
      "Average loss epoch 11: 0.2780425654901361\n",
      "Average loss epoch 12: 0.2762427282377061\n",
      "Average loss epoch 13: 0.2743015103722838\n",
      "Average loss epoch 14: 0.2730051168445159\n",
      "Average loss epoch 15: 0.2715776440990242\n",
      "Average loss epoch 16: 0.2702519554014233\n",
      "Average loss epoch 17: 0.26909441262558104\n",
      "Average loss epoch 18: 0.26803176822662356\n",
      "Average loss epoch 19: 0.26697464186962355\n",
      "Average loss epoch 20: 0.26593426075936716\n",
      "Average loss epoch 21: 0.2648775556875901\n",
      "Average loss epoch 22: 0.26418952169922943\n",
      "Average loss epoch 23: 0.2632855111617256\n",
      "Average loss epoch 24: 0.2628226162211123\n",
      "Average loss epoch 25: 0.2621422074639323\n",
      "Average loss epoch 26: 0.261117001602968\n",
      "Average loss epoch 27: 0.26063566993288695\n",
      "Average loss epoch 28: 0.25990387130874126\n",
      "Average loss epoch 29: 0.259574918859452\n",
      "Total time: 71.5365800857544 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9258\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter batch_size from 128 to 8\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 8\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.0130203073113566\n",
      "Average loss epoch 1: 0.568937481039478\n",
      "Average loss epoch 2: 0.4850011040154103\n",
      "Average loss epoch 3: 0.443768404085922\n",
      "Average loss epoch 4: 0.41890446904166334\n",
      "Average loss epoch 5: 0.40110326710138666\n",
      "Average loss epoch 6: 0.3875665288240724\n",
      "Average loss epoch 7: 0.3770676819700856\n",
      "Average loss epoch 8: 0.3691668362466226\n",
      "Average loss epoch 9: 0.36165978926926745\n",
      "Average loss epoch 10: 0.355075729288794\n",
      "Average loss epoch 11: 0.3507925109098621\n",
      "Average loss epoch 12: 0.34535719094336104\n",
      "Average loss epoch 13: 0.3418629128871891\n",
      "Average loss epoch 14: 0.33828533948231077\n",
      "Average loss epoch 15: 0.33474517409820914\n",
      "Average loss epoch 16: 0.3309859680716595\n",
      "Average loss epoch 17: 0.3287337881548718\n",
      "Average loss epoch 18: 0.3262320451863431\n",
      "Average loss epoch 19: 0.3238108041625778\n",
      "Average loss epoch 20: 0.3218067998415655\n",
      "Average loss epoch 21: 0.31988403053652836\n",
      "Average loss epoch 22: 0.3177778054640235\n",
      "Average loss epoch 23: 0.31611074975608255\n",
      "Average loss epoch 24: 0.31408954446439274\n",
      "Average loss epoch 25: 0.3133094147697733\n",
      "Average loss epoch 26: 0.3116202339946322\n",
      "Average loss epoch 27: 0.3092902851430862\n",
      "Average loss epoch 28: 0.3091703137172531\n",
      "Average loss epoch 29: 0.3067469572056436\n",
      "Total time: 16.866400957107544 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9163\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter batch_size from 128 to 64\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.5991551112905842\n",
      "Average loss epoch 1: 0.988557985452848\n",
      "Average loss epoch 2: 0.7850848505987185\n",
      "Average loss epoch 3: 0.684185985928384\n",
      "Average loss epoch 4: 0.6236556570663631\n",
      "Average loss epoch 5: 0.5801044123195042\n",
      "Average loss epoch 6: 0.5498918559506675\n",
      "Average loss epoch 7: 0.5256863498242101\n",
      "Average loss epoch 8: 0.5053296655973541\n",
      "Average loss epoch 9: 0.4918803930561119\n",
      "Average loss epoch 10: 0.47574225332692405\n",
      "Average loss epoch 11: 0.4668340191384342\n",
      "Average loss epoch 12: 0.4559929606234916\n",
      "Average loss epoch 13: 0.44773855582575933\n",
      "Average loss epoch 14: 0.44084663466315405\n",
      "Average loss epoch 15: 0.4315660036055841\n",
      "Average loss epoch 16: 0.4269054054378349\n",
      "Average loss epoch 17: 0.4204776877554778\n",
      "Average loss epoch 18: 0.4157412196988257\n",
      "Average loss epoch 19: 0.41179990935548444\n",
      "Average loss epoch 20: 0.4057294893487592\n",
      "Average loss epoch 21: 0.402462299739089\n",
      "Average loss epoch 22: 0.4004311025421196\n",
      "Average loss epoch 23: 0.3945448819164918\n",
      "Average loss epoch 24: 0.3911269563380803\n",
      "Average loss epoch 25: 0.3907452176943004\n",
      "Average loss epoch 26: 0.38647683090138657\n",
      "Average loss epoch 27: 0.3807302260788802\n",
      "Average loss epoch 28: 0.3816532143087031\n",
      "Average loss epoch 29: 0.3765833338844442\n",
      "Total time: 10.881861209869385 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9035\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter batch_size from 128 to 256\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vary parameter learning_rate through values 0.001, 0.005, 0.01, 0.02 and 0.05\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 2.098208599990898\n",
      "Average loss epoch 1: 1.7660632786495147\n",
      "Average loss epoch 2: 1.5261980817590282\n",
      "Average loss epoch 3: 1.3499627752459689\n",
      "Average loss epoch 4: 1.2181070556729545\n",
      "Average loss epoch 5: 1.1168109364720769\n",
      "Average loss epoch 6: 1.0370189307175037\n",
      "Average loss epoch 7: 0.9744652952903357\n",
      "Average loss epoch 8: 0.9204825439375319\n",
      "Average loss epoch 9: 0.8775712408663787\n",
      "Average loss epoch 10: 0.8400648330752944\n",
      "Average loss epoch 11: 0.807970299309506\n",
      "Average loss epoch 12: 0.7803042097525164\n",
      "Average loss epoch 13: 0.7558730158494624\n",
      "Average loss epoch 14: 0.7339077415444079\n",
      "Average loss epoch 15: 0.7137373428800445\n",
      "Average loss epoch 16: 0.6973551684048348\n",
      "Average loss epoch 17: 0.6818443960521049\n",
      "Average loss epoch 18: 0.6674261047289922\n",
      "Average loss epoch 19: 0.6549313454639106\n",
      "Average loss epoch 20: 0.6413794029981662\n",
      "Average loss epoch 21: 0.6310611282473122\n",
      "Average loss epoch 22: 0.6203778473647324\n",
      "Average loss epoch 23: 0.6115975058301067\n",
      "Average loss epoch 24: 0.6015937568007649\n",
      "Average loss epoch 25: 0.5952573890313679\n",
      "Average loss epoch 26: 0.5870274015656718\n",
      "Average loss epoch 27: 0.5774181722066342\n",
      "Average loss epoch 28: 0.5730779493327464\n",
      "Average loss epoch 29: 0.565321579054519\n",
      "Total time: 16.844319105148315 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.8758\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter learning_rate from 0.01 to 0.001\n",
    "## Define paramaters for the model\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.5892761945724487\n",
      "Average loss epoch 1: 0.984506179124881\n",
      "Average loss epoch 2: 0.7832934370129814\n",
      "Average loss epoch 3: 0.683280596991519\n",
      "Average loss epoch 4: 0.6217026457642064\n",
      "Average loss epoch 5: 0.5795309185842812\n",
      "Average loss epoch 6: 0.549273625259355\n",
      "Average loss epoch 7: 0.5248713975463992\n",
      "Average loss epoch 8: 0.5064683576008101\n",
      "Average loss epoch 9: 0.4898436744035263\n",
      "Average loss epoch 10: 0.4763194542108994\n",
      "Average loss epoch 11: 0.46667193741231533\n",
      "Average loss epoch 12: 0.45545211246797257\n",
      "Average loss epoch 13: 0.44682703297454995\n",
      "Average loss epoch 14: 0.4393581197812007\n",
      "Average loss epoch 15: 0.43316867273726384\n",
      "Average loss epoch 16: 0.42592268181847526\n",
      "Average loss epoch 17: 0.42032049537260774\n",
      "Average loss epoch 18: 0.4156460616833124\n",
      "Average loss epoch 19: 0.41084619988372556\n",
      "Average loss epoch 20: 0.40798599844351235\n",
      "Average loss epoch 21: 0.4013710391896588\n",
      "Average loss epoch 22: 0.39849201494302505\n",
      "Average loss epoch 23: 0.3959536850452423\n",
      "Average loss epoch 24: 0.39152418438232306\n",
      "Average loss epoch 25: 0.38647257538386437\n",
      "Average loss epoch 26: 0.38795238823601697\n",
      "Average loss epoch 27: 0.38275786029014275\n",
      "Average loss epoch 28: 0.37989827677920146\n",
      "Average loss epoch 29: 0.37752358278472387\n",
      "Total time: 17.142939567565918 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9028\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter learning_rate from 0.01 to 0.005\n",
    "## Define paramaters for the model\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.0075711669066014\n",
      "Average loss epoch 1: 0.5689360523001575\n",
      "Average loss epoch 2: 0.48461247553358544\n",
      "Average loss epoch 3: 0.444163366770133\n",
      "Average loss epoch 4: 0.41841356883515846\n",
      "Average loss epoch 5: 0.4010674108714213\n",
      "Average loss epoch 6: 0.38863672395964044\n",
      "Average loss epoch 7: 0.3761043419023772\n",
      "Average loss epoch 8: 0.3696700484255255\n",
      "Average loss epoch 9: 0.36208992425378383\n",
      "Average loss epoch 10: 0.353909644769344\n",
      "Average loss epoch 11: 0.3518871026578205\n",
      "Average loss epoch 12: 0.3458653011105277\n",
      "Average loss epoch 13: 0.34118799007300177\n",
      "Average loss epoch 14: 0.338527660200368\n",
      "Average loss epoch 15: 0.33437509942443777\n",
      "Average loss epoch 16: 0.33137093317675426\n",
      "Average loss epoch 17: 0.3294930921428965\n",
      "Average loss epoch 18: 0.32580782552976983\n",
      "Average loss epoch 19: 0.3234915582241712\n",
      "Average loss epoch 20: 0.3226316847376057\n",
      "Average loss epoch 21: 0.319972111315994\n",
      "Average loss epoch 22: 0.31678215881466587\n",
      "Average loss epoch 23: 0.3167253670506266\n",
      "Average loss epoch 24: 0.31425274234690587\n",
      "Average loss epoch 25: 0.31293738527453585\n",
      "Average loss epoch 26: 0.3132576745151084\n",
      "Average loss epoch 27: 0.3079134285936267\n",
      "Average loss epoch 28: 0.30846271794159097\n",
      "Average loss epoch 29: 0.3087377208165633\n",
      "Total time: 17.47498631477356 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9172\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter learning_rate from 0.01 to 0.02\n",
    "## Define paramaters for the model\n",
    "learning_rate = 0.02\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 0.7350461156496079\n",
      "Average loss epoch 1: 0.4408056098264414\n",
      "Average loss epoch 2: 0.39264934272060303\n",
      "Average loss epoch 3: 0.3683220288067153\n",
      "Average loss epoch 4: 0.3530982679628826\n",
      "Average loss epoch 5: 0.3419375197314994\n",
      "Average loss epoch 6: 0.3336602254903122\n",
      "Average loss epoch 7: 0.3261643698165467\n",
      "Average loss epoch 8: 0.32128395695925316\n",
      "Average loss epoch 9: 0.31614996610960483\n",
      "Average loss epoch 10: 0.31350765535325714\n",
      "Average loss epoch 11: 0.308317118973443\n",
      "Average loss epoch 12: 0.3078526063954636\n",
      "Average loss epoch 13: 0.3027571209105023\n",
      "Average loss epoch 14: 0.29920525334792814\n",
      "Average loss epoch 15: 0.3005336325242247\n",
      "Average loss epoch 16: 0.2964017069214708\n",
      "Average loss epoch 17: 0.2947552597328222\n",
      "Average loss epoch 18: 0.29362719281986877\n",
      "Average loss epoch 19: 0.2923746019393414\n",
      "Average loss epoch 20: 0.2901765892416725\n",
      "Average loss epoch 21: 0.2892842849234601\n",
      "Average loss epoch 22: 0.2879775716187237\n",
      "Average loss epoch 23: 0.28594484115952934\n",
      "Average loss epoch 24: 0.28527745257168663\n",
      "Average loss epoch 25: 0.2836329137389754\n",
      "Average loss epoch 26: 0.2854416463480685\n",
      "Average loss epoch 27: 0.28111364272160405\n",
      "Average loss epoch 28: 0.28130415710674855\n",
      "Average loss epoch 29: 0.28154012609472917\n",
      "Total time: 17.752582550048828 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9207\n"
     ]
    }
   ],
   "source": [
    "# I modify parameter learning_rate from 0.01 to 0.05\n",
    "## Define paramaters for the model\n",
    "learning_rate = 0.05\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
